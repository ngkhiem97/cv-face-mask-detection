digraph {
	graph [size="64.05,64.05"]
	node [align=left fontname=monospace fontsize=10 height=0.2 ranksep=0.1 shape=box style=filled]
	139834042841344 [label="
 (32, 2)" fillcolor=darkolivegreen1]
	139834042966224 [label=AddmmBackward0]
	139834042969488 -> 139834042966224
	139832751723920 [label="classifier.6.bias
 (2)" fillcolor=lightblue]
	139832751723920 -> 139834042969488
	139834042969488 [label=AccumulateGrad]
	139834042969440 -> 139834042966224
	139834042969440 [label=ReluBackward0]
	139834042966512 -> 139834042969440
	139834042966512 [label=AddmmBackward0]
	139834042969776 -> 139834042966512
	139832751725120 [label="classifier.4.bias
 (2048)" fillcolor=lightblue]
	139832751725120 -> 139834042969776
	139834042969776 [label=AccumulateGrad]
	139834042968864 -> 139834042966512
	139834042968864 [label=MulBackward0]
	139834042966560 -> 139834042968864
	139834042966560 [label=ReluBackward0]
	139834042966464 -> 139834042966560
	139834042966464 [label=AddmmBackward0]
	139834042969680 -> 139834042966464
	139832751724480 [label="classifier.1.bias
 (2048)" fillcolor=lightblue]
	139832751724480 -> 139834042969680
	139834042969680 [label=AccumulateGrad]
	139834042966608 -> 139834042966464
	139834042966608 [label=MulBackward0]
	139834042967328 -> 139834042966608
	139834042967328 [label=ViewBackward0]
	139834042969968 -> 139834042967328
	139834042969968 [label=AvgPool2DBackward0]
	139834042966800 -> 139834042969968
	139834042966800 [label=ReluBackward0]
	139834042968384 -> 139834042966800
	139834042968384 [label=AddBackward0]
	139834042968672 -> 139834042968384
	139834042968672 [label=NativeBatchNormBackward0]
	139834042969872 -> 139834042968672
	139834042969872 [label=ConvolutionBackward0]
	139834042967472 -> 139834042969872
	139834042967472 [label=ReluBackward0]
	139834042966176 -> 139834042967472
	139834042966176 [label=NativeBatchNormBackward0]
	139834042969920 -> 139834042966176
	139834042969920 [label=ConvolutionBackward0]
	139834042969248 -> 139834042969920
	139834042969248 [label=ReluBackward0]
	139834042966080 -> 139834042969248
	139834042966080 [label=AddBackward0]
	139834042969536 -> 139834042966080
	139834042969536 [label=NativeBatchNormBackward0]
	139834042969200 -> 139834042969536
	139834042969200 [label=ConvolutionBackward0]
	139834042996336 -> 139834042969200
	139834042996336 [label=ReluBackward0]
	139832751686464 -> 139834042996336
	139832751686464 [label=NativeBatchNormBackward0]
	139832751686656 -> 139832751686464
	139832751686656 [label=ConvolutionBackward0]
	139832751688480 -> 139832751686656
	139832751688480 [label=ReluBackward0]
	139832751688816 -> 139832751688480
	139832751688816 [label=AddBackward0]
	139832751689392 -> 139832751688816
	139832751689392 [label=NativeBatchNormBackward0]
	139832751687568 -> 139832751689392
	139832751687568 [label=ConvolutionBackward0]
	139832751687328 -> 139832751687568
	139832751687328 [label=ReluBackward0]
	139832751687616 -> 139832751687328
	139832751687616 [label=NativeBatchNormBackward0]
	139832751689440 -> 139832751687616
	139832751689440 [label=ConvolutionBackward0]
	139832751685840 -> 139832751689440
	139832751685840 [label=ReluBackward0]
	139832639652240 -> 139832751685840
	139832639652240 [label=AddBackward0]
	139832639655024 -> 139832639652240
	139832639655024 [label=NativeBatchNormBackward0]
	139832639655264 -> 139832639655024
	139832639655264 [label=ConvolutionBackward0]
	139832639654592 -> 139832639655264
	139832639654592 [label=ReluBackward0]
	139832639652624 -> 139832639654592
	139832639652624 [label=NativeBatchNormBackward0]
	139832639652480 -> 139832639652624
	139832639652480 [label=ConvolutionBackward0]
	139832639652960 -> 139832639652480
	139832639652960 [label=ReluBackward0]
	139832639652816 -> 139832639652960
	139832639652816 [label=AddBackward0]
	139832639653392 -> 139832639652816
	139832639653392 [label=NativeBatchNormBackward0]
	139832639652432 -> 139832639653392
	139832639652432 [label=ConvolutionBackward0]
	139832639654832 -> 139832639652432
	139832639654832 [label=ReluBackward0]
	139832639655456 -> 139832639654832
	139832639655456 [label=NativeBatchNormBackward0]
	139832639653584 -> 139832639655456
	139832639653584 [label=ConvolutionBackward0]
	139832639654928 -> 139832639653584
	139832639654928 [label=ReluBackward0]
	139832639654544 -> 139832639654928
	139832639654544 [label=AddBackward0]
	139832639654688 -> 139832639654544
	139832639654688 [label=NativeBatchNormBackward0]
	139832639655072 -> 139832639654688
	139832639655072 [label=ConvolutionBackward0]
	139832639654016 -> 139832639655072
	139832639654016 [label=ReluBackward0]
	139832639652336 -> 139832639654016
	139832639652336 [label=NativeBatchNormBackward0]
	139832639654304 -> 139832639652336
	139832639654304 [label=ConvolutionBackward0]
	139832639651904 -> 139832639654304
	139832639651904 [label=ReluBackward0]
	139832639654208 -> 139832639651904
	139832639654208 [label=AddBackward0]
	139832639654160 -> 139832639654208
	139832639654160 [label=NativeBatchNormBackward0]
	139832639654496 -> 139832639654160
	139832639654496 [label=ConvolutionBackward0]
	139832639655696 -> 139832639654496
	139832639655696 [label=ReluBackward0]
	139832639653488 -> 139832639655696
	139832639653488 [label=NativeBatchNormBackward0]
	139834042838224 -> 139832639653488
	139834042838224 [label=ConvolutionBackward0]
	139832639654112 -> 139834042838224
	139832639654112 [label=ReluBackward0]
	139834042838368 -> 139832639654112
	139834042838368 [label=AddBackward0]
	139834042838560 -> 139834042838368
	139834042838560 [label=NativeBatchNormBackward0]
	139834042838080 -> 139834042838560
	139834042838080 [label=ConvolutionBackward0]
	139834042837024 -> 139834042838080
	139834042837024 [label=ReluBackward0]
	139834042837600 -> 139834042837024
	139834042837600 [label=NativeBatchNormBackward0]
	139834042837168 -> 139834042837600
	139834042837168 [label=ConvolutionBackward0]
	139834042838608 -> 139834042837168
	139834042838608 [label=ReluBackward0]
	139834042836304 -> 139834042838608
	139834042836304 [label=NativeBatchNormBackward0]
	139834042835200 -> 139834042836304
	139834042835200 [label=ConvolutionBackward0]
	139834042836928 -> 139834042835200
	139832639616320 [label="conv1.weight
 (64, 3, 3, 3)" fillcolor=lightblue]
	139832639616320 -> 139834042836928
	139834042836928 [label=AccumulateGrad]
	139834042836832 -> 139834042836304
	139832639617120 [label="bn1.weight
 (64)" fillcolor=lightblue]
	139832639617120 -> 139834042836832
	139834042836832 [label=AccumulateGrad]
	139834042835632 -> 139834042836304
	139832639617040 [label="bn1.bias
 (64)" fillcolor=lightblue]
	139832639617040 -> 139834042835632
	139834042835632 [label=AccumulateGrad]
	139834042835728 -> 139834042837168
	139832639617600 [label="layer1.0.conv1.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	139832639617600 -> 139834042835728
	139834042835728 [label=AccumulateGrad]
	139834042836784 -> 139834042837600
	139832639615920 [label="layer1.0.bn1.weight
 (64)" fillcolor=lightblue]
	139832639615920 -> 139834042836784
	139834042836784 [label=AccumulateGrad]
	139834042837216 -> 139834042837600
	139832639618160 [label="layer1.0.bn1.bias
 (64)" fillcolor=lightblue]
	139832639618160 -> 139834042837216
	139834042837216 [label=AccumulateGrad]
	139834042837408 -> 139834042838080
	139832639618000 [label="layer1.0.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	139832639618000 -> 139834042837408
	139834042837408 [label=AccumulateGrad]
	139834042838656 -> 139834042838560
	139832639618240 [label="layer1.0.bn2.weight
 (64)" fillcolor=lightblue]
	139832639618240 -> 139834042838656
	139834042838656 [label=AccumulateGrad]
	139834042835296 -> 139834042838560
	139832639618320 [label="layer1.0.bn2.bias
 (64)" fillcolor=lightblue]
	139832639618320 -> 139834042835296
	139834042835296 [label=AccumulateGrad]
	139834042838608 -> 139834042838368
	139834042838320 -> 139834042838224
	139834042981408 [label="layer1.1.conv1.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	139834042981408 -> 139834042838320
	139834042838320 [label=AccumulateGrad]
	139834042838512 -> 139832639653488
	139834042981488 [label="layer1.1.bn1.weight
 (64)" fillcolor=lightblue]
	139834042981488 -> 139834042838512
	139834042838512 [label=AccumulateGrad]
	139834042838800 -> 139832639653488
	139834042980608 [label="layer1.1.bn1.bias
 (64)" fillcolor=lightblue]
	139834042980608 -> 139834042838800
	139834042838800 [label=AccumulateGrad]
	139832639654880 -> 139832639654496
	139834042981648 [label="layer1.1.conv2.weight
 (64, 64, 3, 3)" fillcolor=lightblue]
	139834042981648 -> 139832639654880
	139832639654880 [label=AccumulateGrad]
	139832639654352 -> 139832639654160
	139834042980848 [label="layer1.1.bn2.weight
 (64)" fillcolor=lightblue]
	139834042980848 -> 139832639654352
	139832639654352 [label=AccumulateGrad]
	139832639652768 -> 139832639654160
	139834042981568 [label="layer1.1.bn2.bias
 (64)" fillcolor=lightblue]
	139834042981568 -> 139832639652768
	139832639652768 [label=AccumulateGrad]
	139832639654112 -> 139832639654208
	139832639653056 -> 139832639654304
	139834042978448 [label="layer2.0.conv1.weight
 (128, 64, 3, 3)" fillcolor=lightblue]
	139834042978448 -> 139832639653056
	139832639653056 [label=AccumulateGrad]
	139832639653296 -> 139832639652336
	139832818565344 [label="layer2.0.bn1.weight
 (128)" fillcolor=lightblue]
	139832818565344 -> 139832639653296
	139832639653296 [label=AccumulateGrad]
	139832639654784 -> 139832639652336
	139832751696288 [label="layer2.0.bn1.bias
 (128)" fillcolor=lightblue]
	139832751696288 -> 139832639654784
	139832639654784 [label=AccumulateGrad]
	139832639652000 -> 139832639655072
	139832639547968 [label="layer2.0.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	139832639547968 -> 139832639652000
	139832639652000 [label=AccumulateGrad]
	139832639652720 -> 139832639654688
	139832639547728 [label="layer2.0.bn2.weight
 (128)" fillcolor=lightblue]
	139832639547728 -> 139832639652720
	139832639652720 [label=AccumulateGrad]
	139832639652912 -> 139832639654688
	139832639547648 [label="layer2.0.bn2.bias
 (128)" fillcolor=lightblue]
	139832639547648 -> 139832639652912
	139832639652912 [label=AccumulateGrad]
	139832639654736 -> 139832639654544
	139832639654736 [label=NativeBatchNormBackward0]
	139832639652528 -> 139832639654736
	139832639652528 [label=ConvolutionBackward0]
	139832639651904 -> 139832639652528
	139832639653728 -> 139832639652528
	139832639548528 [label="layer2.0.shortcut.0.weight
 (128, 64, 1, 1)" fillcolor=lightblue]
	139832639548528 -> 139832639653728
	139832639653728 [label=AccumulateGrad]
	139832639652048 -> 139832639654736
	139834042991056 [label="layer2.0.shortcut.1.weight
 (128)" fillcolor=lightblue]
	139834042991056 -> 139832639652048
	139832639652048 [label=AccumulateGrad]
	139832639652864 -> 139832639654736
	139834042990976 [label="layer2.0.shortcut.1.bias
 (128)" fillcolor=lightblue]
	139834042990976 -> 139832639652864
	139832639652864 [label=AccumulateGrad]
	139832639653104 -> 139832639653584
	139834042994496 [label="layer2.1.conv1.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	139834042994496 -> 139832639653104
	139832639653104 [label=AccumulateGrad]
	139832639653632 -> 139832639655456
	139834042992096 [label="layer2.1.bn1.weight
 (128)" fillcolor=lightblue]
	139834042992096 -> 139832639653632
	139832639653632 [label=AccumulateGrad]
	139832639654640 -> 139832639655456
	139834042992736 [label="layer2.1.bn1.bias
 (128)" fillcolor=lightblue]
	139834042992736 -> 139832639654640
	139832639654640 [label=AccumulateGrad]
	139832639653440 -> 139832639652432
	139834043076672 [label="layer2.1.conv2.weight
 (128, 128, 3, 3)" fillcolor=lightblue]
	139834043076672 -> 139832639653440
	139832639653440 [label=AccumulateGrad]
	139832639653248 -> 139832639653392
	139834043079632 [label="layer2.1.bn2.weight
 (128)" fillcolor=lightblue]
	139834043079632 -> 139832639653248
	139832639653248 [label=AccumulateGrad]
	139832639655216 -> 139832639653392
	139834043079072 [label="layer2.1.bn2.bias
 (128)" fillcolor=lightblue]
	139834043079072 -> 139832639655216
	139832639655216 [label=AccumulateGrad]
	139832639654928 -> 139832639652816
	139832639653680 -> 139832639652480
	139834043080112 [label="layer3.0.conv1.weight
 (256, 128, 3, 3)" fillcolor=lightblue]
	139834043080112 -> 139832639653680
	139832639653680 [label=AccumulateGrad]
	139832639652672 -> 139832639652624
	139834043079472 [label="layer3.0.bn1.weight
 (256)" fillcolor=lightblue]
	139834043079472 -> 139832639652672
	139832639652672 [label=AccumulateGrad]
	139832639654256 -> 139832639652624
	139834043080352 [label="layer3.0.bn1.bias
 (256)" fillcolor=lightblue]
	139834043080352 -> 139832639654256
	139832639654256 [label=AccumulateGrad]
	139832639652144 -> 139832639655264
	139834043078592 [label="layer3.0.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	139834043078592 -> 139832639652144
	139832639652144 [label=AccumulateGrad]
	139832639653776 -> 139832639655024
	139834043078032 [label="layer3.0.bn2.weight
 (256)" fillcolor=lightblue]
	139834043078032 -> 139832639653776
	139832639653776 [label=AccumulateGrad]
	139832639655408 -> 139832639655024
	139834043077312 [label="layer3.0.bn2.bias
 (256)" fillcolor=lightblue]
	139834043077312 -> 139832639655408
	139832639655408 [label=AccumulateGrad]
	139832639652096 -> 139832639652240
	139832639652096 [label=NativeBatchNormBackward0]
	139832639653200 -> 139832639652096
	139832639653200 [label=ConvolutionBackward0]
	139832639652960 -> 139832639653200
	139832639655312 -> 139832639653200
	139834043111520 [label="layer3.0.shortcut.0.weight
 (256, 128, 1, 1)" fillcolor=lightblue]
	139834043111520 -> 139832639655312
	139832639655312 [label=AccumulateGrad]
	139832639652192 -> 139832639652096
	139834043110800 [label="layer3.0.shortcut.1.weight
 (256)" fillcolor=lightblue]
	139834043110800 -> 139832639652192
	139832639652192 [label=AccumulateGrad]
	139832639651952 -> 139832639652096
	139834043109680 [label="layer3.0.shortcut.1.bias
 (256)" fillcolor=lightblue]
	139834043109680 -> 139832639651952
	139832639651952 [label=AccumulateGrad]
	139832751689344 -> 139832751689440
	139834043110480 [label="layer3.1.conv1.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	139834043110480 -> 139832751689344
	139832751689344 [label=AccumulateGrad]
	139832751687184 -> 139832751687616
	139834043112240 [label="layer3.1.bn1.weight
 (256)" fillcolor=lightblue]
	139834043112240 -> 139832751687184
	139832751687184 [label=AccumulateGrad]
	139832751689680 -> 139832751687616
	139834043112480 [label="layer3.1.bn1.bias
 (256)" fillcolor=lightblue]
	139834043112480 -> 139832751689680
	139832751689680 [label=AccumulateGrad]
	139832751686608 -> 139832751687568
	139834043551072 [label="layer3.1.conv2.weight
 (256, 256, 3, 3)" fillcolor=lightblue]
	139834043551072 -> 139832751686608
	139832751686608 [label=AccumulateGrad]
	139832751687232 -> 139832751689392
	139834043551312 [label="layer3.1.bn2.weight
 (256)" fillcolor=lightblue]
	139834043551312 -> 139832751687232
	139832751687232 [label=AccumulateGrad]
	139832751685936 -> 139832751689392
	139834043550672 [label="layer3.1.bn2.bias
 (256)" fillcolor=lightblue]
	139834043550672 -> 139832751685936
	139832751685936 [label=AccumulateGrad]
	139832751685840 -> 139832751688816
	139832751688048 -> 139832751686656
	139834043112320 [label="layer4.0.conv1.weight
 (512, 256, 3, 3)" fillcolor=lightblue]
	139834043112320 -> 139832751688048
	139832751688048 [label=AccumulateGrad]
	139832751685696 -> 139832751686464
	139834043113040 [label="layer4.0.bn1.weight
 (512)" fillcolor=lightblue]
	139834043113040 -> 139832751685696
	139832751685696 [label=AccumulateGrad]
	139832751689248 -> 139832751686464
	139834043113280 [label="layer4.0.bn1.bias
 (512)" fillcolor=lightblue]
	139834043113280 -> 139832751689248
	139832751689248 [label=AccumulateGrad]
	139834042968960 -> 139834042969200
	139832751697488 [label="layer4.0.conv2.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	139832751697488 -> 139834042968960
	139834042968960 [label=AccumulateGrad]
	139834042966656 -> 139834042969536
	139834043077232 [label="layer4.0.bn2.weight
 (512)" fillcolor=lightblue]
	139834043077232 -> 139834042966656
	139834042966656 [label=AccumulateGrad]
	139834042968240 -> 139834042969536
	139834043078832 [label="layer4.0.bn2.bias
 (512)" fillcolor=lightblue]
	139834043078832 -> 139834042968240
	139834042968240 [label=AccumulateGrad]
	139834042966752 -> 139834042966080
	139834042966752 [label=NativeBatchNormBackward0]
	139834042966128 -> 139834042966752
	139834042966128 [label=ConvolutionBackward0]
	139832751688480 -> 139834042966128
	139832751686224 -> 139834042966128
	139834042742640 [label="layer4.0.shortcut.0.weight
 (512, 256, 1, 1)" fillcolor=lightblue]
	139834042742640 -> 139832751686224
	139832751686224 [label=AccumulateGrad]
	139832751688288 -> 139834042966752
	139834042742720 [label="layer4.0.shortcut.1.weight
 (512)" fillcolor=lightblue]
	139834042742720 -> 139832751688288
	139832751688288 [label=AccumulateGrad]
	139832751686320 -> 139834042966752
	139834042741280 [label="layer4.0.shortcut.1.bias
 (512)" fillcolor=lightblue]
	139834042741280 -> 139832751686320
	139832751686320 [label=AccumulateGrad]
	139834042966416 -> 139834042969920
	139834042744480 [label="layer4.1.conv1.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	139834042744480 -> 139834042966416
	139834042966416 [label=AccumulateGrad]
	139834042969728 -> 139834042966176
	139834042741120 [label="layer4.1.bn1.weight
 (512)" fillcolor=lightblue]
	139834042741120 -> 139834042969728
	139834042969728 [label=AccumulateGrad]
	139834042967280 -> 139834042966176
	139834042741360 [label="layer4.1.bn1.bias
 (512)" fillcolor=lightblue]
	139834042741360 -> 139834042967280
	139834042967280 [label=AccumulateGrad]
	139834042966704 -> 139834042969872
	139834042741840 [label="layer4.1.conv2.weight
 (512, 512, 3, 3)" fillcolor=lightblue]
	139834042741840 -> 139834042966704
	139834042966704 [label=AccumulateGrad]
	139834042969104 -> 139834042968672
	139834042742160 [label="layer4.1.bn2.weight
 (512)" fillcolor=lightblue]
	139834042742160 -> 139834042969104
	139834042969104 [label=AccumulateGrad]
	139834042970016 -> 139834042968672
	139834042742400 [label="layer4.1.bn2.bias
 (512)" fillcolor=lightblue]
	139834042742400 -> 139834042970016
	139834042970016 [label=AccumulateGrad]
	139834042969248 -> 139834042968384
	139834042967376 -> 139834042966464
	139834042967376 [label=TBackward0]
	139834042967040 -> 139834042967376
	139834042742080 [label="classifier.1.weight
 (2048, 25088)" fillcolor=lightblue]
	139834042742080 -> 139834042967040
	139834042967040 [label=AccumulateGrad]
	139834042967184 -> 139834042966512
	139834042967184 [label=TBackward0]
	139834042967136 -> 139834042967184
	139832751725760 [label="classifier.4.weight
 (2048, 2048)" fillcolor=lightblue]
	139832751725760 -> 139834042967136
	139834042967136 [label=AccumulateGrad]
	139834042967088 -> 139834042966224
	139834042967088 [label=TBackward0]
	139834042968528 -> 139834042967088
	139832751724560 [label="classifier.6.weight
 (2, 2048)" fillcolor=lightblue]
	139832751724560 -> 139834042968528
	139834042968528 [label=AccumulateGrad]
	139834042966224 -> 139834042841344
}
